{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapsewsp01"
		},
		"AzureDataLakeStorage_generaladls2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage_generaladls2'"
		},
		"AzureSql_sample_sql_instance_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSql_sample_sql_instance'"
		},
		"linkedService_synapse_sample_lake_db_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'linkedService_synapse_sample_lake_db'"
		},
		"linkedService_synapse_sample_sales_dw_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'linkedService_synapse_sample_sales_dw'"
		},
		"synapsewsp01-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapsewsp01-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapsewsp01.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"tempjsonconnector_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'tempjsonconnector'"
		},
		"AzureBlobStorage_TaxiSample_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'AzureBlobStorage_TaxiSample'"
		},
		"AzureDataLakeStorage_generaladls2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://generaladls2.dfs.core.windows.net/"
		},
		"AzureKeyVault_kv_t01_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://kv-t01.vault.azure.net/"
		},
		"synapsewsp01-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsews01adls2.dfs.core.windows.net"
		},
		"tempjsonconnector_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://generaladls2.dfs.core.windows.net/"
		},
		"Trigger 1_properties_typeProperties_scope": {
			"type": "string",
			"defaultValue": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-storages-adlsgen2/providers/Microsoft.Storage/storageAccounts/generaladls2"
		},
		"trg_storage_event_properties_pipe_avrofile_process_test_parameters_pipe_src_path": {
			"type": "string",
			"defaultValue": "@triggerBody().folderPath"
		},
		"trg_storage_event_properties_typeProperties_scope": {
			"type": "string",
			"defaultValue": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-storages-adlsgen2/providers/Microsoft.Storage/storageAccounts/generaladls2"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CopyPipeline_TaxiSampleData')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_2pr",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "public-parquet/taxi/"
							},
							{
								"name": "Destination",
								"value": "row-data/TaxiSampleData/"
							}
						],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false,
							"enableSkipIncompatibleRow": false,
							"skipErrorFile": {
								"fileMissing": true,
								"dataInconsistency": false
							},
							"validateDataConsistency": true,
							"logSettings": {
								"enableCopyActivityLog": true,
								"copyActivityLogSettings": {
									"logLevel": "Warning",
									"enableReliableLogging": false
								},
								"logLocationSettings": {
									"linkedServiceName": {
										"referenceName": "synapsewsp01-WorkspaceDefaultStorage",
										"type": "LinkedServiceReference"
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_nyctaxidata",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset__nyctaxidata",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "sample-copy-activites"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/SourceDataset_nyctaxidata')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset__nyctaxidata')]",
				"[concat(variables('workspaceId'), '/linkedServices/synapsewsp01-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseResumeDedicatedPools')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GET List",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:10:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools?api-version=2019-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.azure.com/"
							}
						}
					},
					{
						"name": "Filter Prod",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "GET List",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get list').output.value",
								"type": "Expression"
							},
							"condition": {
								"value": "@not(endswith(item().name,'prod'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach_pool",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Filter Prod",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter Prod').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CheckState",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',item().name,'?api-version=2019-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.azure.com/"
										}
									}
								},
								{
									"name": "State-PauseOrResume",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "CheckState",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@concat(activity('CheckState').output.properties.status,'-',pipeline().parameters.PauseOrResume)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "Paused-Resume",
												"activities": [
													{
														"name": "WebPostResumeCommand",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/resume?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											},
											{
												"value": "Online-Pause",
												"activities": [
													{
														"name": "WebPostPauseCommand",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/pause?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "test-synapse-analytics"
					},
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "3c321efc-9b78-45cd-ab39-1378151cbd42"
					},
					"WorkspaceName": {
						"type": "string",
						"defaultValue": "synapsewsp01"
					},
					"SQLPoolName": {
						"type": "string",
						"defaultValue": "sample__sales_dw"
					},
					"PauseorResume": {
						"type": "string",
						"defaultValue": "Pause"
					}
				},
				"folder": {
					"name": "utility"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pipe_avrofile_process')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "https://github.com/balakreshnan/Samples2021/blob/main/ADF/avroprocessdf.md",
				"activities": [
					{
						"name": "df-process-avro-files",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_avrofile_process_sample",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_avrofile_process_sample')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pipe_avrofile_process_lifeworks_sample')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "https://github.com/balakreshnan/Samples2021/blob/main/ADF/avroprocessdf.md",
				"activities": [
					{
						"name": "df-process-avro-files",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_avrofile_process_lifeworks_sample",
								"type": "DataFlowReference",
								"parameters": {
									"source_path": {
										"value": "'@{concat('row-data/sample_avro_data'    \n    ,formatDateTime(addMinutes(utcNow(),-10),'yyyy/MM/dd/HH/mm'))}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"sourceaescafiles": {},
									"sinkavrotocsv": {},
									"sinkAvroToCache": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "None",
							"cacheSinks": {}
						}
					},
					{
						"name": "Set variable Avro file row written to csv",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "df-process-avro-files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "avro_rowread_count",
							"value": {
								"value": "@string(activity('df-process-avro-files').output.runStatus.metrics.sinkavrotocsv.rowsWritten)",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"avro_rowread_arr": {
						"type": "Array"
					},
					"sql_rowwritten_arr": {
						"type": "Array"
					},
					"avro_rowread_count": {
						"type": "String"
					},
					"sql_rowwritten_count": {
						"type": "String"
					}
				},
				"folder": {
					"name": "lifeworks"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_avrofile_process_lifeworks_sample')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pipe_avrofile_process_test')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "https://github.com/balakreshnan/Samples2021/blob/main/ADF/avroprocessdf.md",
				"activities": [
					{
						"name": "df-process-avro-files",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_avrofile_process_lifeworks_test",
								"type": "DataFlowReference",
								"parameters": {
									"source_path": {
										"value": "'@{concat('row-data/sample_avro_data'    \n    ,formatDateTime(addMinutes(utcNow(),-10),'yyyy/MM/dd/HH/mm'))}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"sourceaescafiles": {},
									"sinkavrotocsv": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "None",
							"cacheSinks": {}
						}
					},
					{
						"name": "Set variable Avro file row written to csv",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "df-process-avro-files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "csv_rowwritten_count",
							"value": {
								"value": "@string(activity('df-process-avro-files').output.runStatus.metrics.sinkavrotocsv.rowsWritten)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "If Condition1",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Set variable Avro file row written to csv",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@greater(int(variables('csv_rowwritten_count')),0)",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Set variable1",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "rowcopiedthan0",
										"value": {
											"value": "@concat('row copied less than=0',\nactivity('df-process-avro-files').Error.Message\n)",
											"type": "Expression"
										}
									}
								}
							],
							"ifTrueActivities": [
								{
									"name": "rowcopiedthan0",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "rowcopiedthan0",
										"value": {
											"value": "@concat('copy actvity copied rows than 0 error message is',\nactivity('df-process-avro-files').Error.Message\n)",
											"type": "Expression"
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"pipe_src_path": {
						"type": "string",
						"defaultValue": "'nnn'"
					}
				},
				"variables": {
					"avro_rowread_arr": {
						"type": "Array"
					},
					"csv_rowwritten_arr": {
						"type": "Array"
					},
					"avro_rowread_count": {
						"type": "String"
					},
					"csv_rowwritten_count": {
						"type": "String"
					},
					"rowcopiedthan0": {
						"type": "String"
					}
				},
				"folder": {
					"name": "lifeworks"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_avrofile_process_lifeworks_test')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pipe_copy_lake_tbl_to_dedicated_pool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "df_copy_lake_to_dedicated_pool",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "get_lake_table_list",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_copy_lake_to_dedicated_pool",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {
								"linkedService": {
									"referenceName": "AzureDataLakeStorage_generaladls2",
									"type": "LinkedServiceReference"
								},
								"folderPath": "copy-row-data"
							},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "None",
							"cacheSinks": {
								"firstRowOnly": true
							}
						}
					},
					{
						"name": "get_lake_table_list",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": "SELECT TABLE_SCHEMA,TABLE_NAME from INFORMATION_SCHEMA.TABLES",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "ds_AzureSynapseAnalytics_lake_db",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "sample-copy-activites"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/df_copy_lake_to_dedicated_pool')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]",
				"[concat(variables('workspaceId'), '/datasets/ds_AzureSynapseAnalytics_lake_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Avro1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage_generaladls2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Avro",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "row-data"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage_generaladls2",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Customers_Rogers.csv",
						"folderPath": "sample_csv_data",
						"fileSystem": "row-data"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset__nyctaxidata')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapsewsp01-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "TaxiSampleData",
						"fileSystem": "row-data"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapsewsp01-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_nyctaxidata')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage_TaxiSample",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "taxi",
						"container": "public-parquet"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage_TaxiSample')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_AzureSynapseAnalytics_lake_db')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "linkedService_synapse_sample_lake_db",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService_synapse_sample_lake_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage_TaxiSample')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('AzureBlobStorage_TaxiSample_sasUri')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage_generaladls2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage_generaladls2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage_generaladls2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault_kv_t01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault_kv_t01_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSql_sample_sql_instance')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSql_sample_sql_instance_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/linkedService_synapse_sample_lake_db')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('linkedService_synapse_sample_lake_db_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/linkedService_synapse_sample_sales_dw')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('linkedService_synapse_sample_sales_dw_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsewsp01-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapsewsp01-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsewsp01-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapsewsp01-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tempjsonconnector')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('tempjsonconnector_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('tempjsonconnector_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger 1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pipe_avrofile_process_lifeworks_sample",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "BlobEventsTrigger",
				"typeProperties": {
					"blobPathBeginsWith": "/row-data/blobs/sample_avro_data/",
					"blobPathEndsWith": "22.avro",
					"ignoreEmptyBlobs": true,
					"scope": "[parameters('Trigger 1_properties_typeProperties_scope')]",
					"events": [
						"Microsoft.Storage.BlobCreated"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pipe_avrofile_process_lifeworks_sample')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trg_storage_event')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "pipe_avrofile_process_test",
							"type": "PipelineReference"
						},
						"parameters": {
							"pipe_src_path": "[parameters('trg_storage_event_properties_pipe_avrofile_process_test_parameters_pipe_src_path')]"
						}
					}
				],
				"type": "BlobEventsTrigger",
				"typeProperties": {
					"blobPathBeginsWith": "/row-data/blobs/sample_avro_data/",
					"blobPathEndsWith": "*.avro",
					"ignoreEmptyBlobs": true,
					"scope": "[parameters('trg_storage_event_properties_typeProperties_scope')]",
					"events": [
						"Microsoft.Storage.BlobCreated"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/pipe_avrofile_process_test')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-SSIS')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "Canada Central",
						"nodeSize": "Standard_D8_v3",
						"numberOfNodes": 1,
						"maxParallelExecutionsPerNode": 8
					},
					"ssisProperties": {
						"catalogInfo": {
							"catalogServerEndpoint": "sample-sql-instance.database.windows.net",
							"catalogAdminUserName": "saadmin",
							"catalogAdminPassword": {
								"type": "SecureString",
								"value": "**********"
							},
							"catalogPricingTier": "S1"
						},
						"edition": "Standard",
						"licenseType": "BasePrice"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IROnPrem')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_avrofile_process_lifeworks_sample')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "\n\nwildcard pattern -- \n\n41_ctr_ca_04042023_*",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "AzureDataLakeStorage_generaladls2",
								"type": "LinkedServiceReference"
							},
							"name": "sourceaescafiles"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "AzureDataLakeStorage_generaladls2",
								"type": "LinkedServiceReference"
							},
							"name": "sinkavrotocsv"
						},
						{
							"name": "sinkAvroToCache"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "select1"
						},
						{
							"name": "parse1"
						},
						{
							"name": "parse2"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "SplitCsvAndCache"
						}
					],
					"scriptLines": [
						"parameters{",
						"     source_path as string ('none')",
						"}",
						"source(output(",
						"          SequenceNumber as long,",
						"          Offset as string,",
						"          EnqueuedTimeUtc as string,",
						"          SystemProperties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Properties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Body as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'avro',",
						"     fileSystem: 'row-data',",
						"     folderPath: 'sample_avro_data',",
						"     fileName: '46_aes_ca_04042023_1925.avro') ~> sourceaescafiles",
						"parse1 foldDown(unroll(newbody.Records, newbody.Records),",
						"     mapColumn(",
						"          data = newbody.Records.kinesis.data",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"parse2 select(mapColumn(",
						"          data",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"sourceaescafiles parse(newbody = Body ? (Records as ",
						"",
						"(kinesis as ",
						"",
						"(kinesisSchemaVersion as string,",
						"     partitionKey as string,",
						"     sequenceNumber as string,",
						"     data as string,",
						"     approximateArrivalTimestamp as integer)",
						"     )[])[],",
						"     format: 'json',",
						"     documentForm: 'documentPerLine') ~> parse1",
						"derivedColumn1 parse(data = data ? (AWSAccountId as string,",
						"     AgentARN as string,",
						"     CurrentAgentSnapshot as (AgentStatus as (ARN as string,",
						"     Name as string,",
						"     StartTimestamp as string,",
						"     Type as string),",
						"     Configuration as (AgentHierarchyGroups as string,",
						"     AutoAccept as boolean,",
						"     FirstName as string,",
						"     LanguageCode as string,",
						"     LastName as string,",
						"     ProficiencyList as string ,",
						"     RoutingProfile as (ARN as string,",
						"     Concurrency as (AvailableSlots as integer,",
						"     Channel as string,",
						"     MaximumSlots as integer)[],",
						"     DefaultOutboundQueue as (ARN as string,",
						"     ",
						"     Name as string),",
						"     InboundQueues as (ARN as string,",
						"     ",
						"     Name as string)[],",
						"     Name as string),",
						"     SipAddress as string,",
						"     Username as string),",
						"     Contacts as (Channel as string,",
						"     ConnectedToAgentTimestamp as string,",
						"     ContactId as string,",
						"     InitialContactId as string,",
						"     InitiationMethod as string,",
						"     Queue as (ARN as string,",
						"     Name as string),",
						"     QueueTimestamp as string ,",
						"     State as string,",
						"     StateStartTimestamp as string)[],",
						"     NextAgentStatus as string ),",
						"     EventId as string,",
						"     EventTimestamp as string,",
						"     EventType as string,",
						"     InstanceARN as string,",
						"     PreviousAgentSnapshot as (AgentStatus as (ARN as string,",
						"     Name as string,",
						"     StartTimestamp as string,",
						"     Type as string),",
						"     Configuration as (AgentHierarchyGroups as string,",
						"     AutoAccept as boolean,",
						"     FirstName as string,",
						"     LanguageCode as string,",
						"     LastName as string,",
						"     ProficiencyList as string ,",
						"     RoutingProfile as (ARN as string,",
						"     Concurrency as (AvailableSlots as integer,",
						"     Channel as string,",
						"     MaximumSlots as integer)[],",
						"     DefaultOutboundQueue as (ARN as string,",
						"     Name as string),",
						"     InboundQueues as (ARN as string,",
						"     ",
						"     Name as string)[],",
						"     Name as string),",
						"     SipAddress as string,",
						"     Username as string),",
						"     Contacts as (Channel as string,",
						"     ConnectedToAgentTimestamp as string,",
						"     ContactId as string,",
						"     InitialContactId as string,",
						"     InitiationMethod as string,",
						"     Queue as (ARN as string,",
						"     Name as string),",
						"     QueueTimestamp as string,",
						"     State as string,",
						"     StateStartTimestamp as string)[],",
						"     NextAgentStatus as string),",
						"     Version as string)[],",
						"     format: 'json',",
						"     documentForm: 'documentPerLine') ~> parse2",
						"flatten1 derive(data = fromBase64(data)) ~> derivedColumn1",
						"select1 split(true(),",
						"     true(),",
						"     disjoint: true) ~> SplitCsvAndCache@(sinkAvroToCsv, sinkAvroToCache)",
						"SplitCsvAndCache@sinkAvroToCsv sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delimited',",
						"     fileSystem: 'row-data',",
						"     folderPath: 'sample_csv_data',",
						"     columnDelimiter: ',',",
						"     escapeChar: '\\\\',",
						"     quoteChar: '\\\"',",
						"     columnNamesAsHeader: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2) ~> sinkavrotocsv",
						"SplitCsvAndCache@sinkAvroToCache sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sinkAvroToCache"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_avrofile_process_lifeworks_test')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "\n\nwildcard pattern -- \n\n41_ctr_ca_04042023_*",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "AzureDataLakeStorage_generaladls2",
								"type": "LinkedServiceReference"
							},
							"name": "sourceaescafiles"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "AzureDataLakeStorage_generaladls2",
								"type": "LinkedServiceReference"
							},
							"name": "sinkavrotocsv"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "select1"
						},
						{
							"name": "parse1"
						},
						{
							"name": "parse2"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     source_path as string ('none')",
						"}",
						"source(output(",
						"          SequenceNumber as long,",
						"          Offset as string,",
						"          EnqueuedTimeUtc as string,",
						"          SystemProperties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Properties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Body as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'avro',",
						"     fileSystem: 'row-data',",
						"     folderPath: 'sample_avro_data',",
						"     fileName: '46_aes_ca_04042023_1925.avro') ~> sourceaescafiles",
						"parse1 foldDown(unroll(newbody.Records, newbody.Records),",
						"     mapColumn(",
						"          data = newbody.Records.kinesis.data",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"parse2 select(mapColumn(",
						"          AWSAccountId = data[1].AWSAccountId",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"sourceaescafiles parse(newbody = Body ? (Records as ",
						"",
						"(kinesis as ",
						"",
						"(kinesisSchemaVersion as string,",
						"     partitionKey as string,",
						"     sequenceNumber as string,",
						"     data as string,",
						"     approximateArrivalTimestamp as integer)",
						"     )[])[],",
						"     format: 'json',",
						"     documentForm: 'documentPerLine') ~> parse1",
						"derivedColumn1 parse(data = data ? (AWSAccountId as string,",
						"    AgentARN as string,",
						"    CurrentAgentSnapshot as (AgentStatus as (ARN as string,Name as string,StartTimestamp as string,Type as string),",
						"    Configuration as (AgentHierarchyGroups as (Level1 as (ARN as string,Name as string),",
						"    Level2 as (ARN as string,",
						"    Name as string),",
						"    Level3 as (ARN as string,",
						"    Name as string),",
						"    Level4 as (ARN as string,",
						"    Name as string),",
						"    Level5 as (ARN as string,",
						"    Name as string)),",
						"    AutoAccept as boolean,",
						"    FirstName as string,",
						"    LanguageCode as string,",
						"    LastName as string,",
						"    ProficiencyList as string,",
						"    RoutingProfile as (ARN as string,",
						"    Concurrency as (AvailableSlots as integer,",
						"    Channel as string,",
						"    MaximumSlots as integer)[],",
						"    DefaultOutboundQueue as (ARN as string,",
						"    Channels as (dd as string,",
						"    ee as string,",
						"    rr as string,",
						"    tt as string,",
						"    yy as string)[],",
						"    Name as string),",
						"    InboundQueues as (ARN as string,",
						"    Channels as (dd as string,",
						"    ee as string,",
						"    rr as string,",
						"    tt as string,",
						"    yy as string)[],",
						"    Name as string)[],",
						"    Name as string),",
						"    SipAddress as string,",
						"    Username as string),",
						"    Contacts as (Channel as string,",
						"    ConnectedToAgentTimestamp as string,",
						"    ContactId as string,",
						"    InitialContactId as string,",
						"    InitiationMethod as string,",
						"    Queue as (ARN as string,",
						"    Name as string",
						"    ),",
						"    QueueTimestamp as string,",
						"    State as string,",
						"    StateStartTimestamp as string",
						"    )[],",
						"    NextAgentStatus as string),",
						"    EventId as string,",
						"    EventTimestamp as string,",
						"    EventType as string,",
						"    InstanceARN as string,",
						"    PreviousAgentSnapshot as (AgentStatus as (ARN as string,",
						"    Name as string,",
						"    StartTimestamp as string,",
						"    Type as string),",
						"    Configuration as (AgentHierarchyGroups as (Level1 as (ARN as string,",
						"    Name as string),",
						"    Level2 as (ARN as string,",
						"    Name as string),",
						"    Level3 as (ARN as string,",
						"    Name as string),",
						"    Level4 as (ARN as string,",
						"    Name as string),",
						"    Level5 as (ARN as string,",
						"    Name as string) ),",
						"    AutoAccept as boolean,",
						"    FirstName as string,",
						"    LanguageCode as string,",
						"    LastName as string,",
						"    ProficiencyList as string,",
						"    RoutingProfile as (ARN as string,",
						"    Concurrency as (AvailableSlots as integer,",
						"    Channel as string,",
						"    MaximumSlots as integer)[],",
						"    DefaultOutboundQueue as (ARN as string,",
						"   Channels as (dd as string,",
						"    ee as string,",
						"    rr as string,",
						"    tt as string,",
						"    yy as string)[],",
						"    Name as string),",
						"    InboundQueues as (ARN as string,",
						"    Channels as (dd as string,",
						"    ee as string,",
						"    rr as string,",
						"    tt as string,",
						"    yy as string)[],",
						"    Name as string)[],",
						"    Name as string),",
						"    SipAddress as string,",
						"    Username as string),",
						"    Contacts as (Channel as string,",
						"    ConnectedToAgentTimestamp as string,",
						"    ContactId as string,",
						"    InitialContactId as string,",
						"    InitiationMethod as string,",
						"    Queue as (ARN as string,",
						"    Name as string),",
						"    QueueTimestamp as string,",
						"    State as string,",
						"    StateStartTimestamp as string",
						"    )[],",
						"    NextAgentStatus as string),",
						"    Version as string)[],",
						"     format: 'json',",
						"     documentForm: 'documentPerLine') ~> parse2",
						"flatten1 derive(data = fromBase64(data)) ~> derivedColumn1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delimited',",
						"     fileSystem: 'row-data',",
						"     folderPath: 'sample_csv_data',",
						"     columnDelimiter: ',',",
						"     escapeChar: '\\\\',",
						"     quoteChar: '\\\"',",
						"     columnNamesAsHeader: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 1) ~> sinkavrotocsv"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_avrofile_process_sample')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "AzureDataLakeStorage_generaladls2",
								"type": "LinkedServiceReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parse1"
						},
						{
							"name": "flatten1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          SequenceNumber as long,",
						"          Offset as string,",
						"          EnqueuedTimeUtc as string,",
						"          SystemProperties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Properties as [string,(member0 as long, member1 as double, member2 as string, member3 as binary)],",
						"          Body as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'avro',",
						"     fileSystem: 'row-data',",
						"     folderPath: 'sample_avro_data',",
						"     fileName: '22.avro') ~> source1",
						"source1 parse(NewBody = Body ? (applicationId as string,",
						"     messageSource as string,",
						"     deviceId as string,",
						"     schema as string,",
						"     templateId as string,",
						"     enqueuedTime as string,",
						"     telemetry as (temperature as integer,pressure as integer,magnetometer as (x as integer,",
						"     y as integer,     z as integer),",
						"     gyroscope as (x as integer,",
						"     y as integer,",
						"     z as integer),",
						"     accelerometer as (x as integer,",
						"     y as integer,",
						"     z as integer),",
						"     humidity as integer)",
						"     )[],",
						"     format: 'json',",
						"     documentForm: 'documentPerLine') ~> parse1",
						"parse1 foldDown(unroll(NewBody),",
						"     mapColumn(",
						"          applicationId = NewBody.applicationId,",
						"          messageSource = NewBody.messageSource,",
						"          deviceId = NewBody.deviceId,",
						"          schema = NewBody.schema,",
						"          templateId = NewBody.templateId,",
						"          enqueuedTime = NewBody.enqueuedTime,",
						"          pressure = NewBody.telemetry.pressure",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 select(mapColumn(",
						"          applicationId,",
						"          messageSource,",
						"          deviceId,",
						"          schema,",
						"          templateId,",
						"          enqueuedTime,",
						"          pressure",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Column_1 as string,",
						"          Column_2 as string,",
						"          Column_3 as string,",
						"          Column_4 as string,",
						"          Column_5 as string,",
						"          Column_6 as string,",
						"          Column_7 as string,",
						"          Column_8 as string,",
						"          Column_9 as string,",
						"          Column_10 as string,",
						"          Column_11 as string,",
						"          Column_12 as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage_generaladls2')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/df_copy_lake_to_dedicated_pool')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "source1"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "linkedService_synapse_sample_sales_dw",
								"type": "LinkedServiceReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     store: 'synapse',",
						"     databaseType: 'spark',",
						"     format: 'table',",
						"     database: 'sample_lake_db',",
						"     tableName: 'tbl1') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'synapseanalytics',",
						"     schemaName: 'dbo',",
						"     tableName: 'tbl1',",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     allowCopyCommand: true,",
						"     staged: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService_synapse_sample_sales_dw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create external table with SQL Serverless')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--CREATE database sample_serverless_nyctaxidata\n\n/* Note: this script is filtered on a specific month. You can modify the location to read the entire dataset. */\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat')\n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat]\n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'nyctlc_azureopendatastorage_blob_core_windows_net')\n\tCREATE EXTERNAL DATA SOURCE [nyctlc_azureopendatastorage_blob_core_windows_net]\n\tWITH (\n\t\tLOCATION   = 'https://azureopendatastorage.blob.core.windows.net/nyctlc',\n\t)\nGo\n\nCREATE EXTERNAL TABLE nyc_tlc_yellow_trip_ext (\n\t[vendorID] varchar(8000),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] varchar(8000),\n\t[doLocationId] varchar(8000),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] varchar(8000),\n\t[paymentType] varchar(8000),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] varchar(8000),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float\n\t)\n\tWITH (\n    LOCATION = 'yellow/puYear=2014/puMonth=3/*.parquet',\n\t-- LOCATION = 'yellow/puYear=*/puMonth=*/*.parquet'\n\tDATA_SOURCE = [nyctlc_azureopendatastorage_blob_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nSELECT TOP 100 * FROM nyc_tlc_yellow_trip_ext\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sample_serverless_nyctaxidata",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Bulk Completion_Copy')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "882c73a0-c3d9-463a-b091-08a82e70ebd1"
					}
				},
				"metadata": {
					"saveOutput": false,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-synapse-analytics/providers/Microsoft.Synapse/workspaces/synapsewsp01/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://synapsewsp01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Azure Open AI Summarization Use case\r\n",
							"\r\n",
							"This notebook provides an example code showing how to use the Open AI packages for the bulk chat completion for summarization use case. The verbatim dataset is loaded from Azure datalake which contains the verbaige for the specific plant operation defect condition. We are going to summarize that through Open AI GPT3.5 LLM and store the result in the enriched dataset back to datalake."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Required imports\r\n",
							"import openai\r\n",
							"import re\r\n",
							"import pandas as pd\r\n",
							"import numpy as np\r\n",
							"import json\r\n",
							"import requests"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Setup Azure OpenAI environment\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_key = \"1369d83861a247fb9e2113c9f177dc21\"\r\n",
							"openai.api_base = \"https://resturant-review-classification.openai.azure.com/\"\r\n",
							"openai.api_version = \"2023-03-15-preview\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Mount Datalake path for accessing datasets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Update parameters for your environment\r\n",
							"storage_account = 'generaladls2'\r\n",
							"container_name = 'row-data'\r\n",
							"subfolder = 'sample_csv_data'\r\n",
							"linkedServiceName = \"AzureDataLakeStorage_generaladls2\"\r\n",
							"\r\n",
							"mssparkutils.fs.unmount(\"/openai\")\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{subfolder}/\", \r\n",
							"    \"/openai\", \r\n",
							"    {\"linkedService\":linkedServiceName} \r\n",
							") \r\n",
							"\r\n",
							"jobId = mssparkutils.env.getJobId() \r\n",
							"print(f\"Job Id: {jobId}\")\r\n",
							"\r\n",
							"\r\n",
							"# https://generaladls2.blob.core.windows.net/row-data/sample_csv_data/RestaurantReviews.csv"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(f\"synfs:/{jobId}/openai/\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = pd.read_csv(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{subfolder}/PaintDefectVerbatim.csv\") \r\n",
							"df"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# s is input text\r\n",
							"def normalize_text(s, sep_token = \" \\n \"):\r\n",
							"    s = re.sub(r'\\s+',  ' ', s).strip()\r\n",
							"    s = re.sub(r\". ,\",\"\",s)\r\n",
							"    # remove all instances of multiple spaces\r\n",
							"    s = s.replace(\"..\",\".\")\r\n",
							"    s = s.replace(\". .\",\".\")\r\n",
							"    s = s.replace(\"\\n\", \"\")\r\n",
							"    s = s.strip()\r\n",
							"    \r\n",
							"    return s\r\n",
							"\r\n",
							"df['verbatim_normalized'] = df[\"verbatim\"].apply(lambda x : normalize_text(x))\r\n",
							"df\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class Prompt:\r\n",
							"    def __init__(self, role, content):\r\n",
							"        self.role = role\r\n",
							"        self.content = content\r\n",
							"\r\n",
							"\r\n",
							"sys_prompt = Prompt(\r\n",
							"    role=\"system\",\r\n",
							"    content = \"You are an AI assistant that will help me succinctly summarize the failure in 2 words based on the verbatim I provide. I am looking for failure context which best describes the situation.\"\r\n",
							")\r\n",
							"prompts = list()\r\n",
							"prompts.append(sys_prompt)\r\n",
							"\r\n",
							"def initiate_chat():\r\n",
							"    global prompts, openai\r\n",
							"    response = openai.ChatCompletion.create(\r\n",
							"        engine=\"ChatGPT\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\r\n",
							"        messages = [sys_prompt.__dict__]\r\n",
							"    )\r\n",
							"    result = response['choices'][0]['message']['content']\r\n",
							"    print(f\"Result: {result}\")\r\n",
							"    prompts.append(Prompt(role=\"system\", content=result))\r\n",
							"\r\n",
							"# Initiate the OpenAI engine\r\n",
							"initiate_chat()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def summarize_verbetim(v):\r\n",
							"    global prompts, openai, sys_prompt\r\n",
							"    message = f\"Verbatim: {v} \\n Code: ?\"\r\n",
							"    prompt = Prompt(role=\"user\", content=message)\r\n",
							"    prompts.append(prompt)\r\n",
							"    response = openai.ChatCompletion.create(\r\n",
							"        engine=\"ChatGPT\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\r\n",
							"        messages = [sys_prompt.__dict__, prompt.__dict__]\r\n",
							"    )\r\n",
							"\r\n",
							"    result = response['choices'][0]['message']['content']\r\n",
							"    print(result)\r\n",
							"    prompts.append(Prompt(role=\"system\", content=result))\r\n",
							"\r\n",
							"    return result\r\n",
							"\r\n",
							"\r\n",
							"df['summarization'] = df[\"verbatim_normalized\"].apply(lambda x : summarize_verbetim(x))\r\n",
							"\r\n",
							"df_enriched = df.drop(['verbatim'], axis=1, inplace=False)\r\n",
							"df_enriched"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_enriched.to_csv(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{subfolder}/data/PaintDefectVerbatim_enriched.csv\", sep=',', encoding='utf-8')"
						],
						"outputs": [],
						"execution_count": 64
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices - OpenAI')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "743bee29-3f78-4e4c-9d03-23fc01d4eb0a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "dd4c8776-6853-4257-bef8-72778724ad57",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"# Azure OpenAI for Big Data\n",
							"\n",
							"The Azure OpenAI service can be used to solve a large number of natural language tasks through prompting the completion API. To make it easier to scale your prompting workflows from a few examples to large datasets of examples we have integrated the Azure OpenAI service with the distributed machine learning library [SynapseML](https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/). This integration makes it easy to use the [Apache Spark](https://spark.apache.org/) distributed computing framework to process millions of prompts with the OpenAI service. This tutorial shows how to apply large language models at a distributed scale using Azure Open AI and Azure Synapse Analytics. \n",
							"\n",
							"## Step 1: Prerequisites\n",
							"\n",
							"The key prerequisites for this quickstart include a working Azure OpenAI resource, and an Apache Spark cluster with SynapseML installed. We suggest creating a Synapse workspace, but an Azure Databricks, HDInsight, or Spark on Kubernetes, or even a python environment with the `pyspark` package will work. \n",
							"\n",
							"1. An Azure OpenAI resource  request access [here](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu) before [creating a resource](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource)\n",
							"1. [Create a Synapse workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace)\n",
							"1. [Create a serverless Apache Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark#create-a-serverless-apache-spark-pool)\n",
							"\n",
							"\n",
							"## Step 2: Import this guide as a notebook\n",
							"\n",
							"The next step is to add this code into your Spark cluster. You can either create a notebook in your Spark platform and copy the code into this notebook to run the demo. Or download the notebook and import it into Synapse Analytics\n",
							"\n",
							"1.\t[Download this demo as a notebook](https://github.com/microsoft/SynapseML/blob/master/notebooks/features/cognitive_services/CognitiveServices%20-%20OpenAI.ipynb) (click Raw, then save the file)\n",
							"1.\tImport the notebook [into the Synapse Workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#create-a-notebook) or if using Databricks [into the Databricks Workspace](https://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage#create-a-notebook)\n",
							"1. Install SynapseML on your cluster. Please see the installation instructions for Synapse at the bottom of [the SynapseML website](https://microsoft.github.io/SynapseML/). Note that this requires pasting an additional cell at the top of the notebook you just imported\n",
							"3.\tConnect your notebook to a cluster and follow along, editing and rnnung the cells below.\n",
							"\n",
							"## Step 3: Fill in your service information\n",
							"\n",
							"Next, please edit the cell in the notebook to point to your service. In particular set the `service_name`, `deployment_name`, `location`, and `key` variables to match those for your OpenAI service:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "1b0db8af-7fe2-40bc-9df4-cc7f274d53f0",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"import os\n",
							"from pyspark.sql import SparkSession\n",
							"from synapse.ml.core.platform import running_on_synapse, find_secret\n",
							"\n",
							"# Bootstrap Spark Session\n",
							"spark = SparkSession.builder.getOrCreate()\n",
							"\n",
							"if running_on_synapse():\n",
							"    from notebookutils.visualization import display\n",
							"\n",
							"# Fill in the following lines with your service information\n",
							"service_name = \"synapseml-openai\"\n",
							"deployment_name = \"gpt-35-turbo\"\n",
							"deployment_name_embeddings = \"text-search-ada-doc-001\"\n",
							"deployment_name_embeddings_query = \"text-search-ada-query-001\"\n",
							"\n",
							"key = find_secret(\"openai-api-key\")  # please replace this with your key as a string\n",
							"\n",
							"assert key is not None and service_name is not None"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "76f069b7-14e8-44ea-97f0-1c49cf02eeed",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"## Step 4: Create a dataset of prompts\n",
							"\n",
							"Next, create a dataframe consisting of a series of rows, with one prompt per row. \n",
							"\n",
							"You can also load data directly from ADLS or other databases. For more information on loading and preparing Spark dataframes, see the [Apache Spark data loading guide](https://spark.apache.org/docs/latest/sql-data-sources.html)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "1a4366df-4d40-45a0-b1a7-6086b9c693d2",
								"showTitle": false,
								"title": ""
							},
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.createDataFrame(\n",
							"    [\n",
							"        (\"Hello my name is\",),\n",
							"        (\"The best code is code thats\",),\n",
							"        (\"SynapseML is \",),\n",
							"    ]\n",
							").toDF(\"prompt\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "fb48578c-e4b3-49fc-9ee2-2f8ae8808c19",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"## Step 5: Create the OpenAICompletion Apache Spark Client\n",
							"\n",
							"To apply the OpenAI Completion service to your dataframe you just created, create an OpenAICompletion object which serves as a distributed client. Parameters of the service can be set either with a single value, or by a column of the dataframe with the appropriate setters on the `OpenAICompletion` object. Here we are setting `maxTokens` to 200. A token is around 4 characters, and this limit applies to the sum of the prompt and the result. We are also setting the `promptCol` parameter with the name of the prompt column in the dataframe."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "2dca7a9d-6092-48af-8653-10c141fd440d",
								"showTitle": false,
								"title": ""
							},
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.cognitive import OpenAICompletion\n",
							"\n",
							"completion = (\n",
							"    OpenAICompletion()\n",
							"    .setSubscriptionKey(key)\n",
							"    .setDeploymentName(deployment_name)\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
							"    .setMaxTokens(200)\n",
							"    .setPromptCol(\"prompt\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setOutputCol(\"completions\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "64b2a454-65ab-45ec-9946-d92539899781",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"## Step 5: Transform the dataframe with the OpenAICompletion Client\n",
							"\n",
							"Now that you have the dataframe and the completion client, you can transform your input dataset and add a column called `completions` with all of the information the service adds. We will select out just the text for simplicity."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "90684f9f-5c74-463f-a809-75a219489d7f",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"from pyspark.sql.functions import col\n",
							"\n",
							"completed_df = completion.transform(df).cache()\n",
							"display(\n",
							"    completed_df.select(\n",
							"        col(\"prompt\"),\n",
							"        col(\"error\"),\n",
							"        col(\"completions.choices.text\").getItem(0).alias(\"text\"),\n",
							"    )\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "d217c822-a213-43b3-aeba-2653e52b3421",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"Your output should look something like this. Please note completion text will be different\n",
							"\n",
							"| **prompt**                 \t| **error** \t| **text**                                                                                                                              \t|\n",
							"|-----------------------------\t|-----------\t|---------------------------------------------------------------------------------------------------------------------------------------\t|\n",
							"| Hello my name is            \t| null      \t| Makaveli I'm eighteen years old and I want to   be a rapper when I grow up I love writing and making music I'm from Los   Angeles, CA \t|\n",
							"| The best code is code thats \t| null      \t| understandable This is a subjective statement,   and there is no definitive answer.                                                   \t|\n",
							"| SynapseML is                \t| null      \t| A machine learning algorithm that is able to learn how to predict the future outcome of events.                                       \t|"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "9111da73-81f2-48e5-9a0c-eb65e1567f91",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"## Additional Usage Examples\n",
							"\n",
							"### Improve throughput with request batching \n",
							"\n",
							"The example above makes several requests to the service, one for each prompt. To complete multiple prompts in a single request, use batch mode. First, in the OpenAICompletion object, instead of setting the Prompt column to \"Prompt\", specify \"batchPrompt\" for the BatchPrompt column.\n",
							"To do so, create a dataframe with a list of prompts per row.\n",
							"\n",
							"**Note** that as of this writing there is currently a limit of 20 prompts in a single request, as well as a hard limit of 2048 \"tokens\", or approximately 1500 words."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "9f9b7953-6d96-4f83-b61d-c396cefb28ea",
								"showTitle": false,
								"title": ""
							},
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"batch_df = spark.createDataFrame(\n",
							"    [\n",
							"        ([\"The time has come\", \"Pleased to\", \"Today stocks\", \"Here's to\"],),\n",
							"        ([\"The only thing\", \"Ask not what\", \"Every litter\", \"I am\"],),\n",
							"    ]\n",
							").toDF(\"batchPrompt\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "0bb5daf9-8155-460c-b2dd-e1ca302a3776",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"Next we create the OpenAICompletion object. Rather than setting the prompt column, set the batchPrompt column if your column is of type `Array[String]`."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "8411e5ba-7f22-4ac9-a78e-1746a7ccc8bc",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"batch_completion = (\n",
							"    OpenAICompletion()\n",
							"    .setSubscriptionKey(key)\n",
							"    .setDeploymentName(deployment_name)\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
							"    .setMaxTokens(200)\n",
							"    .setBatchPromptCol(\"batchPrompt\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setOutputCol(\"completions\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "be2a0d15-40e1-4a3d-a879-d7d0e0129b35",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"In the call to transform a request will then be made per row. Since there are multiple prompts in a single row, each request will be sent with all prompts in that row. The results will contain a row for each row in the request."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "a6fb7509-f582-47bd-8b57-f59d51c03eb3",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"completed_batch_df = batch_completion.transform(batch_df).cache()\n",
							"display(completed_batch_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "2dd7259b-173a-41ef-b98e-7b1dc0df875f",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"### Using an automatic minibatcher\n",
							"\n",
							"If your data is in column format, you can transpose it to row format using SynapseML's `FixedMiniBatcherTransformer`."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "04212778-8002-4e30-bf31-b7511c5776fd",
								"showTitle": false,
								"title": ""
							},
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StringType\n",
							"from synapse.ml.stages import FixedMiniBatchTransformer\n",
							"from synapse.ml.core.spark import FluentAPI\n",
							"\n",
							"completed_autobatch_df = (\n",
							"    df.coalesce(\n",
							"        1\n",
							"    )  # Force a single partition so that our little 4-row dataframe makes a batch of size 4, you can remove this step for large datasets\n",
							"    .mlTransform(FixedMiniBatchTransformer(batchSize=4))\n",
							"    .withColumnRenamed(\"prompt\", \"batchPrompt\")\n",
							"    .mlTransform(batch_completion)\n",
							")\n",
							"\n",
							"display(completed_autobatch_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "f1611cd5-1af9-458f-a1d5-b1f517194cc8",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"### Prompt engineering for translation\n",
							"\n",
							"The Azure OpenAI service can solve many different natural language tasks through [prompt engineering](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions). Here we show an example of prompting for language translation:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "0cee629b-240d-411f-a067-9f7b9ac6ce5d",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"translate_df = spark.createDataFrame(\n",
							"    [\n",
							"        (\"Japanese: Ookina hako \\nEnglish: Big box \\nJapanese: Midori tako\\nEnglish:\",),\n",
							"        (\n",
							"            \"French: Quel heure et il au Montreal? \\nEnglish: What time is it in Montreal? \\nFrench: Ou est le poulet? \\nEnglish:\",\n",
							"        ),\n",
							"    ]\n",
							").toDF(\"prompt\")\n",
							"\n",
							"display(completion.transform(translate_df))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "3a726683-8e19-4ebf-8244-bde82ec4fdbe",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"### Prompt for question answering\n",
							"\n",
							"Here, we prompt GPT-3 for general-knowledge question answering:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"nuid": "831abdac-4b6c-4b7d-b99a-43dcb75c4169",
								"showTitle": false,
								"title": ""
							}
						},
						"source": [
							"qa_df = spark.createDataFrame(\n",
							"    [\n",
							"        (\n",
							"            \"Q: Where is the Grand Canyon?\\nA: The Grand Canyon is in Arizona.\\n\\nQ: What is the weight of the Burj Khalifa in kilograms?\\nA:\",\n",
							"        )\n",
							"    ]\n",
							").toDF(\"prompt\")\n",
							"\n",
							"display(completion.transform(qa_df))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# OpenAI Embeddings\n",
							"\n",
							"We will use t-SNE to reduce the dimensionality of the embeddings from 1536 to 2. Once the embeddings are reduced to two dimensions, we can plot them in a 2D scatter plot."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from synapse.ml.cognitive import OpenAIEmbedding\n",
							"\n",
							"embedding = (\n",
							"    OpenAIEmbedding()\n",
							"    .setSubscriptionKey(key)\n",
							"    .setDeploymentName(deployment_name_embeddings)\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
							"    .setTextCol(\"combined\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setOutputCol(\"embeddings\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import pyspark.sql.functions as F\n",
							"\n",
							"df = spark.read.options(inferSchema=\"True\", delimiter=\",\", header=True).csv(\n",
							"    \"wasbs://publicwasb@mmlspark.blob.core.windows.net/fine_food_reviews_1k.csv\"\n",
							")\n",
							"\n",
							"df = df.withColumn(\n",
							"    \"combined\",\n",
							"    F.format_string(\"Title: %s; Content: %s\", F.trim(df.Summary), F.trim(df.Text)),\n",
							")\n",
							"\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col\n",
							"\n",
							"completed_df = embedding.transform(df).cache()\n",
							"display(completed_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Retrieve embeddings"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"\n",
							"matrix = np.array(completed_df.select(\"embeddings\").collect())[:, 0, :]\n",
							"matrix.shape"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Reduce dimensionality\n",
							"We reduce the dimensionality to 2 dimensions using t-SNE decomposition."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"from sklearn.manifold import TSNE\n",
							"import numpy as np\n",
							"\n",
							"# Create a t-SNE model and transform the data\n",
							"tsne = TSNE(\n",
							"    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\n",
							")\n",
							"vis_dims = tsne.fit_transform(matrix)\n",
							"vis_dims.shape"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Plot the embeddings\n",
							"We colour each review by its star rating, ranging from red for negative reviews, to green for positive reviews..\n",
							"\n",
							"We can observe a decent data separation even in the reduced 2 dimensions."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"import matplotlib\n",
							"import numpy as np\n",
							"\n",
							"scores = np.array(completed_df.select(\"Score\").collect()).reshape(-1)\n",
							"\n",
							"colors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\n",
							"x = [x for x, y in vis_dims]\n",
							"y = [y for x, y in vis_dims]\n",
							"color_indices = scores - 1\n",
							"\n",
							"colormap = matplotlib.colors.ListedColormap(colors)\n",
							"plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
							"for score in [0, 1, 2, 3, 4]:\n",
							"    avg_x = np.array(x)[scores - 1 == score].mean()\n",
							"    avg_y = np.array(y)[scores - 1 == score].mean()\n",
							"    color = colors[score]\n",
							"    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
							"\n",
							"plt.title(\"Amazon ratings visualized in language using t-SNE\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Use embeddings to build a semantic search Index\n",
							"\n",
							"Note that for some OpenAI models, users should use separate models for embedding documents and queries. These models are denoted by the \"-doc\" and \"-query\" suffixes respectively. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"embedding_query = (\n",
							"    OpenAIEmbedding()\n",
							"    .setSubscriptionKey(key)\n",
							"    .setDeploymentName(deployment_name_embeddings_query)\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
							"    .setTextCol(\"query\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setOutputCol(\"embeddings\")\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a dataframe of search queries\n",
							"\n",
							"Note: The data types of the ID columns in the document and query dataframes should be the same"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"query_df = (\n",
							"    spark.createDataFrame(\n",
							"        [\n",
							"            (\n",
							"                0,\n",
							"                \"desserts\",\n",
							"            ),\n",
							"            (\n",
							"                1,\n",
							"                \"disgusting\",\n",
							"            ),\n",
							"        ]\n",
							"    )\n",
							"    .toDF(\"id\", \"query\")\n",
							"    .withColumn(\"id\", F.col(\"id\").cast(\"int\"))\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generate embeddings for queries"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"completed_query_df = embedding_query.transform(query_df).cache()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Build index for fast retrieval"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from synapse.ml.nn import *\n",
							"\n",
							"knn = (\n",
							"    KNN()\n",
							"    .setFeaturesCol(\"embeddings\")\n",
							"    .setValuesCol(\"id\")\n",
							"    .setOutputCol(\"output\")\n",
							"    .setK(10)\n",
							")  # top-k for retrieval\n",
							"\n",
							"knn_index = knn.fit(completed_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Retrieve results"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_matches = knn_index.transform(completed_query_df).cache()\n",
							"\n",
							"df_result = (\n",
							"    df_matches.withColumn(\"match\", F.explode(\"output\"))\n",
							"    .join(df, df[\"id\"] == F.col(\"match.value\"))\n",
							"    .select(\"query\", F.col(\"combined\"), \"match.distance\")\n",
							")\n",
							"\n",
							"display(df_result)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpenAI-Resturant-Classification')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8ad689af-63c2-4a98-ac4f-6226152c2b2b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-synapse-analytics/providers/Microsoft.Synapse/workspaces/synapsewsp01/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://synapsewsp01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from synapse.ml.core.platform import find_secret\r\n",
							"\r\n",
							"# Fill in the following lines with your service information\r\n",
							"service_name = \"rest-review\" # Name of your OpenAI service\r\n",
							"deployment_name = \"text-davinci-002\" # Name of your deployment in OpenAI\r\n",
							"key = find_secret(\"opanaikey\", \"kv-t01\")  # replace this with your secret and keyvault"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"df = spark.read.option('header', 'true') \\\r\n",
							"                .option('delimiter', ',') \\\r\n",
							"                .csv('abfss://row-data@generaladls2.dfs.core.windows.net/sample_csv_data/RestaurantReviews.csv')\r\n",
							"\r\n",
							"display(df)\r\n",
							"\r\n",
							"# https://generaladls2.blob.core.windows.net/row-data/sample_csv_data/RestaurantReviews.csv"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfprompt = df.withColumn(\"prompt\",\\\r\n",
							"                concat(lit(\"Classify the sentiment of following restaurant review. \\n \\\r\n",
							"                Classifications: [Positive, Negative , Neutral] \\n Review:\\\"\\\"\\\" \")\\\r\n",
							"                , col(\"Review\")\\\r\n",
							"                ,lit(\"\\\"\\\"\\\"\\nClassification:\")))\\\r\n",
							"            \r\n",
							"\r\n",
							"display(dfprompt)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.cognitive import OpenAICompletion\r\n",
							"\r\n",
							"completion = (\r\n",
							"    OpenAICompletion()\r\n",
							"    .setSubscriptionKey(key)\r\n",
							"    .setDeploymentName(deployment_name)\r\n",
							"    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
							"    #.setMaxTokens(200)\r\n",
							"    .setPromptCol(\"prompt\")\r\n",
							"    .setErrorCol(\"error\")\r\n",
							"    .setOutputCol(\"response\")\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"completed_df = completion.transform(dfprompt).cache()\r\n",
							"\r\n",
							"display(completed_df)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"display(\r\n",
							"    completed_df.select( \r\n",
							"        col('Restaurant'), \r\n",
							"        col('Review'), \r\n",
							"        col('response.choices.text').getItem(0).alias('openai_sentiment')     \r\n",
							"    )\r\n",
							"    \r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/QueryinyStructuredDataWithOpenAI')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3490d438-9a48-4b59-ae40-788ed9e027b1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-synapse-analytics/providers/Microsoft.Synapse/workspaces/synapsewsp01/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://synapsewsp01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"# Create dataframe with 1000 rows\r\n",
							"df = pd.DataFrame({\r\n",
							"    'Sales Order': np.arange(1, 11),\r\n",
							"    'Order Date': np.random.choice(pd.date_range(start='2020-01-01', end='2022-01-01'), 10),\r\n",
							"    'Customer': [f'Customer {i}' for i in np.random.choice(10, 10)],\r\n",
							"    'Product': [f\"Product {chr(65+i%26)}\" for i in np.random.choice(10,10)],\r\n",
							"    'Quantity': np.random.randint(1, 100, 10),\r\n",
							"    'Price': np.random.randint(10, 1000, 10),\r\n",
							"    'Revenue': np.random.randint(1000, 100000, 10)\r\n",
							"})\r\n",
							"\r\n",
							"# Preview the dataframe\r\n",
							"df.head()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install openai"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import openai\r\n",
							"\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://resturant-review-classification.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"1369d83861a247fb9e2113c9f177dc21\""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pre_text = df.head(10).to_json(orient=\"records\")\r\n",
							"print(pre_text)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"query = input(\"Enter your query in natural language: list the columns name\")\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"text-classification\",\r\n",
							"  prompt=pre_text + \"\\n\" + query,\r\n",
							"  temperature=0.7,\r\n",
							"  max_tokens=2048,\r\n",
							"  top_p=0,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response = response.choices[0].text\r\n",
							"print(\"OpenAI response: \", response)"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/call-purview-api')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "purview-api-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "28f4678d-ffe7-4d70-a574-77c2741c641e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-synapse-analytics/providers/Microsoft.Synapse/workspaces/synapsewsp01/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://synapsewsp01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Reusable Functions\r\n",
							"def azuread_auth(tenant_id: str, client_id: str, client_secret: str, resource_url: str):\r\n",
							"    \"\"\"\r\n",
							"    Authenticates Service Principal to the provided Resource URL, and returns the OAuth Access Token\r\n",
							"    \"\"\"\r\n",
							"    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\r\n",
							"    payload= f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource={resource_url}'\r\n",
							"    headers = {\r\n",
							"    'Content-Type': 'application/x-www-form-urlencoded'\r\n",
							"    }\r\n",
							"    response = requests.request(\"POST\", url, headers=headers, data=payload)\r\n",
							"    access_token = json.loads(response.text)['access_token']\r\n",
							"    return access_token\r\n",
							"\r\n",
							"def purview_auth(tenant_id: str, client_id: str, client_secret: str, data_catalog_name: str):\r\n",
							"    \"\"\"\r\n",
							"    Authenticates to Atlas Endpoint and returns a client object\r\n",
							"    \"\"\"\r\n",
							"    oauth = ServicePrincipalAuthentication(\r\n",
							"        tenant_id = tenant_id,\r\n",
							"        client_id = client_id,\r\n",
							"        client_secret = client_secret\r\n",
							"    )\r\n",
							"    client = PurviewClient(\r\n",
							"        account_name = data_catalog_name,\r\n",
							"        authentication = oauth\r\n",
							"    )\r\n",
							"    return client\r\n",
							"\r\n",
							"def get_all_adls_assets(path: str, data_catalog_name: str, azuread_access_token: str, max_depth=1):\r\n",
							"    \"\"\"\r\n",
							"    Retrieves all scanned assets for the specified ADLS Storage Account Container.\r\n",
							"    \r\n",
							"    Note: this function intentionally recursively traverses until only assets remain (i.e. no folders are returned, only files).\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    # List all files in path\r\n",
							"    url = f\"https://{data_catalog_name}.catalog.purview.azure.com/api/browse?api-version=2022-11-01-preview\"\r\n",
							"\r\n",
							"    headers = {\r\n",
							"            'Authorization': f'Bearer {azuread_access_token}',\r\n",
							"            'Content-Type': 'application/json'\r\n",
							"            }\r\n",
							"\r\n",
							"    payload=\"\"\"{\"limit\": 100,\r\n",
							"                \"offset\": null,\r\n",
							"                \"path\": \"%s\"\r\n",
							"                }\"\"\" % (path)\r\n",
							"                \r\n",
							"    response = requests.request(\"POST\", url, headers=headers, data=payload)\r\n",
							"\r\n",
							"    li = json.loads(response.text)\r\n",
							"\r\n",
							"    # print (li)\r\n",
							"    # Return all files\r\n",
							"    for x in jmespath.search(\"value\", li):\r\n",
							"        if jmespath.search(\"isLeaf\", x):\r\n",
							"            yield x\r\n",
							"\r\n",
							"    # If the max_depth has not been reached, start\r\n",
							"    # listing files and folders in subdirectories\r\n",
							"    if max_depth > 1:\r\n",
							"        for x in jmespath.search(\"value\", li):\r\n",
							"            if jmespath.search(\"isLeaf\", x):\r\n",
							"                continue\r\n",
							"            for y in get_all_adls_assets(jmespath.search(\"path\", x), data_catalog_name, azuread_access_token, max_depth - 1):\r\n",
							"                yield y\r\n",
							"\r\n",
							"    # If max_depth has been reached,\r\n",
							"    # return the folders\r\n",
							"    else:\r\n",
							"        for x in jmespath.search(\"value\", li):\r\n",
							"            if jmespath.search(\"!isLeaf\", x):\r\n",
							"                yield x\r\n",
							"\r\n",
							"def get_adls_asset_schema(assets_all: list, asset: str, purview_client: str):\r\n",
							"    \"\"\"\r\n",
							"    Returns the asset schema and classifications from Purview\r\n",
							"    \"\"\"    \r\n",
							"    # Filter response for our asset of interest\r\n",
							"    assets_list = list(filter(lambda i: i['name'] == asset, assets_all))\r\n",
							"    \r\n",
							"    # Find the guid for the asset to retrieve the tabular_schema or attachedSchema (based on the asset type)\r\n",
							"    match_id = \"\"\r\n",
							"    for entry in assets_list:\r\n",
							"        # Retrieve the asset definition from the Atlas Client\r\n",
							"        response = purview_client.get_entity(entry['id'])\r\n",
							"\r\n",
							"        # API response is different based on the asset\r\n",
							"        if asset.split('.', 1)[-1] == \"json\":\r\n",
							"            filtered_response = jmespath.search(\"entities[?source=='DataScan'].[relationshipAttributes.attachedSchema[0].guid]\", response)\r\n",
							"        else:\r\n",
							"            filtered_response = jmespath.search(\"entities[?source=='DataScan'].[relationshipAttributes.tabular_schema.guid]\", response)\r\n",
							"        \r\n",
							"        # Update match_id if source is DataScan\r\n",
							"        if filtered_response:\r\n",
							"            match_id = filtered_response[0][0]\r\n",
							"    \r\n",
							"    # Retrieve the schema based on the guid match\r\n",
							"    response = purview_client.get_entity(match_id)\r\n",
							"    asset_schema = jmespath.search(\"[referredEntities.*.[attributes.name, classifications[0].[typeName][0]]]\", response)[0]\r\n",
							"\r\n",
							"    return asset_schema\r\n",
							"\r\n",
							"def deep_ls(path: str, max_depth=1):\r\n",
							"    \"\"\"\r\n",
							"    List all files and folders in specified path and\r\n",
							"    subfolders within maximum recursion depth.\r\n",
							"    \"\"\"\r\n",
							"    # List all files in path and apply sorting rules\r\n",
							"    li = mssparkutils.fs.ls(path)\r\n",
							"\r\n",
							"    # Return all files\r\n",
							"    for x in li:\r\n",
							"        if x.size != 0:\r\n",
							"            yield x\r\n",
							"\r\n",
							"    # If the max_depth has not been reached, start\r\n",
							"    # listing files and folders in subdirectories\r\n",
							"    if max_depth > 1:\r\n",
							"        for x in li:\r\n",
							"            if x.size != 0:\r\n",
							"                continue\r\n",
							"            for y in deep_ls(x.path, max_depth - 1):\r\n",
							"                yield y\r\n",
							"\r\n",
							"    # If max_depth has been reached,\r\n",
							"    # return the folders\r\n",
							"    else:\r\n",
							"        for x in li:\r\n",
							"            if x.size == 0:\r\n",
							"                yield x\r\n",
							"\r\n",
							"def convertfiles2df(files):\r\n",
							"    \"\"\"\r\n",
							"    Converts FileInfo object into Pandas DataFrame to enable display\r\n",
							"    \"\"\"\r\n",
							"    # Disable Arrow-based transfers since the Pandas DataFrame is tiny\r\n",
							"    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\r\n",
							"\r\n",
							"    schema = ['path','name','size']\r\n",
							"    df = pd.DataFrame([[getattr(i,j) for j in schema] for i in files], columns = schema).sort_values('path')\r\n",
							"    return(df)\r\n",
							"  \r\n",
							"# Example Implementation\r\n",
							"# ----------------------\r\n",
							"\r\n",
							"# Library Imports\r\n",
							"\r\n",
							"import os\r\n",
							"import requests\r\n",
							"import json\r\n",
							"import jmespath\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pprint import pprint\r\n",
							"from pyapacheatlas.auth import ServicePrincipalAuthentication\r\n",
							"from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess, TypeCategory\r\n",
							"from pyapacheatlas.core.typedef import *\r\n",
							"\r\n",
							"from pyspark.sql import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"# Authentication\r\n",
							"\r\n",
							"# Service Principal with \"Purview Data Source Administrator\" permissions on Purview\r\n",
							"tenant_id = \"16b3c013-d300-468d-ac64-7eda0820b6d3\"\r\n",
							"client_id = \"0cd3895f-c253-40fb-bc87-a3f36abf8196\"\r\n",
							"client_secret = \"a-18Q~s7MNvZQ2Bzl0OQeZif7ai8wWmQ59VAUbgU\"\r\n",
							"resource_url = \"https://purview.azure.net\"\r\n",
							"data_catalog_name = \"testpurview-01\"\r\n",
							"\r\n",
							"# Retrieve authentication objects\r\n",
							"azuread_access_token = azuread_auth(tenant_id, client_id, client_secret, resource_url)\r\n",
							"purview_client = purview_auth(tenant_id, client_id, client_secret, data_catalog_name)\r\n",
							"\r\n",
							"# Asset details\r\n",
							"\r\n",
							"# Asset parameters\r\n",
							"storage_account = \"generaladls2\"\r\n",
							"container = \"row-data\"\r\n",
							"\r\n",
							"# The root level path we want to begin populating assets from\r\n",
							"top_path = f\"/azure_storage_account#{storage_account}.core.windows.net/azure_datalake_gen2_service#{storage_account}.dfs.core.windows.net/azure_datalake_gen2_filesystem#{container}\"\r\n",
							"\r\n",
							"# Retrieve full list of assets\r\n",
							"assets_all = list(get_all_adls_assets(top_path, data_catalog_name, azuread_access_token, max_depth=20))\r\n",
							"\r\n",
							"# Azure storage access info\r\n",
							"linked_service_name = 'AzureDataLakeStorage_generaladls2'\r\n",
							"\r\n",
							"# Grab SAS token\r\n",
							"adls_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
							"\r\n",
							"# Configure Spark to access from DFS endpoint\r\n",
							"root = 'abfss://%s@%s.dfs.core.windows.net/' % (container, storage_account)\r\n",
							"\r\n",
							"spark.conf.set('fs.azure.sas.%s.%s.dfs.core.windows.net' % (container, storage_account), adls_sas_token)\r\n",
							"print('Remote adls root path: ' + root)\r\n",
							"\r\n",
							"# Get ADLS files recursively\r\n",
							"files = list(deep_ls(root, max_depth=20))\r\n",
							"files_df = convertfiles2df(files) # Note this is a Pandas DataFrame\r\n",
							"\r\n",
							"# Generate asset-aligned names\r\n",
							"files_df['asset'] = files_df['name'].str.replace(r'\\d+', '{N}')\r\n",
							"\r\n",
							"# Append schema row-wise from Purview\r\n",
							"files_df['schema'] = files_df.apply(lambda row: get_adls_asset_schema(assets_all, row['asset'], purview_client), axis=1)\r\n",
							"\r\n",
							"# Display Asset DataFrame\r\n",
							"display(files_df)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install jmespath"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install pyapacheatlas"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/read_search_by_name')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "purview-api-test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SampleSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "be48f006-ee2d-4dac-aebc-7774508605f9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/3c321efc-9b78-45cd-ab39-1378151cbd42/resourceGroups/test-synapse-analytics/providers/Microsoft.Synapse/workspaces/synapsewsp01/bigDataPools/SampleSpark",
						"name": "SampleSpark",
						"type": "Spark",
						"endpoint": "https://synapsewsp01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import json\r\n",
							"import os\r\n",
							"\r\n",
							"# PyApacheAtlas packages\r\n",
							"# Connect to Atlas via a Service Principal\r\n",
							"from pyapacheatlas.auth import ServicePrincipalAuthentication\r\n",
							"from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess\r\n",
							"\r\n",
							"if __name__ == \"__main__\":\r\n",
							"    \"\"\"\r\n",
							"    This sample provides an example of searching for an existing entity\r\n",
							"    through the rest api / pyapacheatlas classes.\r\n",
							"    NOTE: This example is specific to Azure Purview's Advanced Search.\r\n",
							"    The response is a Python generator that allows you to page through the\r\n",
							"    search results. For each page in the search results, you have a list\r\n",
							"    of search entities that can be iterated over.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    # Authenticate against your Atlas server\r\n",
							"    oauth = ServicePrincipalAuthentication(\r\n",
							"        tenant_id=os.environ.get(\"TENANT_ID\", \"16b3c013-d300-468d-ac64-7eda0820b6d3\"),\r\n",
							"        client_id=os.environ.get(\"CLIENT_ID\", \"0cd3895f-c253-40fb-bc87-a3f36abf8196\"),\r\n",
							"        client_secret=os.environ.get(\"CLIENT_SECRET\", \"a-18Q~s7MNvZQ2Bzl0OQeZif7ai8wWmQ59VAUbgU\")\r\n",
							"    )\r\n",
							"    client = PurviewClient(\r\n",
							"        account_name = os.environ.get(\"PURVIEW_NAME\", \"testpurview-01\"),\r\n",
							"        authentication=oauth\r\n",
							"    )\r\n",
							"\r\n",
							"    # Assuming you have an entity with the word demo in the name or description\r\n",
							"    search = client.discovery.search_entities(\"Customers_Shaw\")\r\n",
							"    \r\n",
							"    # Alternative search methods include...\r\n",
							"    # Searching across a given attribute:\r\n",
							"    # Search only the name (or qualifiedName) field and it begins with demo\r\n",
							"    # Must include a wildcard character (*) at the end, does not support\r\n",
							"    # wildcard at the beginning or middle.\r\n",
							"\r\n",
							"    #search = client.discovery.search_entities(\"name:Customer*\")\r\n",
							"    #search = client.discovery.search_entities(\"qualifiedName:Phone*\")\r\n",
							"\r\n",
							"    # Searching within a given type and include subtypes...\r\n",
							"    # Provide a search filter that specifies the typeName and whether\r\n",
							"    # you want to include sub types of that type or not.\r\n",
							"\r\n",
							"    # filter_setup = {\"typeName\": \"DataSet\", \"includeSubTypes\": True}\r\n",
							"    # search = client.discovery.search_entities(\"*\", search_filter=filter_setup)\r\n",
							"\r\n",
							"    # The response is a Python generator that allows you to page through the\r\n",
							"    # search results without having to worry about paging or offsets.\r\n",
							"    for entity in search:\r\n",
							"        # Important properties returned include...\r\n",
							"        # id (the guid of the entity), name, qualifedName, @search.score,\r\n",
							"        # and @search.highlights\r\n",
							"        print(json.dumps(entity, indent=2))\r\n",
							"\r\n",
							"\r\n",
							" # ---------------------------------------------------------------\r\n",
							"    print(\"When you know the GUID that you want to get--------------------------------\")\r\n",
							"    # When you know the GUID that you want to get\r\n",
							"    response = client.get_entity(guid=\"df66fec7-c93e-46dd-bd33-5393cb2dc966\")\r\n",
							"    print(json.dumps(response, indent=2))\r\n",
							"\r\n",
							"    # When you need to find multiple Guids and they all are the same type\r\n",
							"    entities = client.get_entity(qualifiedName=[\"https://generaladls2.dfs.core.windows.net/row-data/sample_csv_data/Customers_Shaw.csv\"],\r\n",
							"        typeName=\"azure_datalake_gen2_path\"    )\r\n",
							"    \r\n",
							"    print(\"entities--------------------------------\")\r\n",
							"\r\n",
							"    for entity in entities.get(\"entities\"):\r\n",
							"        print(json.dumps(entity, indent=2))"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install pyapacheatlas"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sample_lake_db')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "sample_lake_db",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://row-data@synapsews01adls2.dfs.core.windows.net/sample_lake_db",
								"Properties": {
									"FormatType": "csv",
									"LinkedServiceName": "synapsewsp01-WorkspaceDefaultStorage"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "tbl1",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "sample_lake_db"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "Id",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "FirstName",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "LastName",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "BirthDate",
										"OriginDataTypeName": {
											"TypeName": "date",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"DateFormat": "YYYY-MM-DD",
												"HIVE_TYPE_STRING": "date"
											}
										}
									},
									{
										"Name": "Email",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Phone",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "SSN",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "AddressLine1",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "AddressLine2",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "City",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "State",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "PinCode",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://row-data@synapsews01adls2.dfs.core.windows.net/Customers_Rogers.csv",
										"delimiter": ",",
										"firstRowAsHeader": "true",
										"multiLine": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "true"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://row-data@synapsews01adls2.dfs.core.windows.net/Customers_Rogers.csv",
									"Properties": {
										"LinkedServiceName": "synapsewsp01-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "tbl_rgr_customers",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "sample_lake_db"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "C1",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C2",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C3",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C4",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C5",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C6",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C7",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C8",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C9",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C10",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C11",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C12",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://row-data@synapsews01adls2.dfs.core.windows.net/Customers_Rogers.csv",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://row-data@synapsews01adls2.dfs.core.windows.net/Customers_Rogers.csv",
									"Properties": {
										"LinkedServiceName": "synapsewsp01-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SampleSpark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "canadacentral"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sample__sales_dw')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "canadacentral"
		}
	]
}